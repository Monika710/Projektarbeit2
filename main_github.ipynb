{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required module\n",
    "from utils.utils import generate_results_csv\n",
    "from utils.utils import create_directory\n",
    "from utils.utils import read_dataset\n",
    "from utils.utils import get_func_length\n",
    "from utils.utils import transform_to_same_length\n",
    "from utils.utils import transform_mts_to_ucr_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required module\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn\n",
    "import utils\n",
    "import classifiers\n",
    "import datetime\n",
    "\n",
    "from utils.constants import CLASSIFIERS\n",
    "from utils.constants import ARCHIVE_NAMES\n",
    "from utils.constants import ITERATIONS\n",
    "from utils.utils import read_all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of fit function\n",
    "def fit_classifier():\n",
    "    x_train = datasets_dict[dataset_name][0]\n",
    "    y_train = datasets_dict[dataset_name][1]\n",
    "    x_test = datasets_dict[dataset_name][2]\n",
    "    y_test = datasets_dict[dataset_name][3]\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension \n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    input_shape = x_train.shape[1:] \n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "    \n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of classifier \n",
    "# We import 2 classifier - FCN and ResNet\n",
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory, verbose=False):\n",
    "    if classifier_name == 'fcn':\n",
    "        from classifiers import fcn\n",
    "        return fcn.Classifier_FCN(output_directory, input_shape, nb_classes, verbose)\n",
    "    if classifier_name == 'resnet':\n",
    "        from classifiers import resnet\n",
    "        return resnet.Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our dataset\n",
    "root_dir = 'D:/FH_Dortmund/Projekt/Projektarbeit_2/time_series/dl-4-tsc-master/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed length and no missing value for univariate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\titer 1\n",
      "\t\t\tdataset_name:  ArrowHead\n",
      "\t\t\t\tDONE\n"
     ]
    }
   ],
   "source": [
    "# main Program for fixed length and no missing value\n",
    "# set the archive_name and import the training dataset\n",
    "# decide the classifier -- fcn or resnet\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'ArrowHead'\n",
    "classifier_name = 'fcn'\n",
    "# classifier_name = 'resnet'\n",
    "itr = '0'\n",
    "\n",
    "# iter = 1 is fcn classifier, iter = 2 is resnet classifier\n",
    "iter = 1\n",
    "\n",
    "# read the training data\n",
    "datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "\n",
    "print('\\t\\titer', iter)\n",
    "trr = ''\n",
    "\n",
    "# create the file about the result\n",
    "if iter != 0:\n",
    "    trr = '_itr_' + str(iter)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "    print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    create_directory(output_directory)\n",
    "\n",
    "\n",
    "# fit the data    \n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "    create_directory(output_directory + '/DONE')\n",
    "\n",
    "else:\n",
    "    print('iter = ', iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 vary length and missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we definition the function read_dataset_original to read the original training data\n",
    "\n",
    "def read_dataset_original(root_dir, archive_name, dataset_name):\n",
    "    datasets_dict = {}\n",
    "    cur_root_dir = root_dir.replace('-temp', '')\n",
    "\n",
    "    if archive_name == 'UCRArchive_2018':\n",
    "       \n",
    "        # print all value \n",
    "        # np.set_printoptions(threshold=np.inf)\n",
    "    \n",
    "        root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
    "        df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.tsv', sep='\\t', header=None)\n",
    "\n",
    "        df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.tsv', sep='\\t', header=None)\n",
    "\n",
    "        y_train = df_train.values[:, 0]\n",
    "        y_test = df_test.values[:, 0]\n",
    "\n",
    "        x_train = df_train.drop(columns=[0])\n",
    "        x_test = df_test.drop(columns=[0])\n",
    "\n",
    "        x_train.columns = range(x_train.shape[1])\n",
    "        x_test.columns = range(x_test.shape[1])\n",
    "\n",
    "        x_train = x_train.values\n",
    "        x_test = x_test.values\n",
    "        \n",
    "        \n",
    "        print(\"original x_train:\", x_train, '\\n')\n",
    "\n",
    "        # znorm\n",
    "        std_ = x_train.std(axis=1, keepdims=True)\n",
    "        std_[std_ == 0] = 1.0\n",
    "        x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n",
    "\n",
    "        std_ = x_test.std(axis=1, keepdims=True)\n",
    "        std_[std_ == 0] = 1.0\n",
    "        x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n",
    "        \n",
    "        print(\"z_norm x_train:\", x_train, '\\n')\n",
    "\n",
    "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                       y_test.copy())\n",
    "    else:\n",
    "        file_name = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/' + dataset_name\n",
    "        x_train, y_train = readucr(file_name + '_TRAIN')\n",
    "        x_test, y_test = readucr(file_name + '_TEST')\n",
    "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                       y_test.copy())\n",
    "\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we definition the function read_dataset_nan to change the nan value \n",
    "\n",
    "def read_dataset_nan(root_dir, archive_name, dataset_name):\n",
    "    datasets_dict = {}\n",
    "    cur_root_dir = root_dir.replace('-temp', '')\n",
    "\n",
    "    if archive_name == 'UCRArchive_2018':\n",
    "        root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
    "        df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.tsv', sep='\\t', header=None)\n",
    "\n",
    "        df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.tsv', sep='\\t', header=None)\n",
    "\n",
    "        y_train = df_train.values[:, 0]\n",
    "        y_test = df_test.values[:, 0]\n",
    "\n",
    "        x_train = df_train.drop(columns=[0])\n",
    "        x_test = df_test.drop(columns=[0])\n",
    "\n",
    "        x_train.columns = range(x_train.shape[1])\n",
    "        x_test.columns = range(x_test.shape[1])\n",
    "\n",
    "        x_train = x_train.values\n",
    "        x_test = x_test.values\n",
    "        \n",
    "        print(\"original x_train:\", x_train, '\\n')\n",
    "\n",
    "        # nan -> 0\n",
    "        \n",
    "        x_train[np.isnan(x_train)] = 0\n",
    "        x_test[np.isnan(x_test)] = 0\n",
    "        \n",
    "        print(\"nan to 0:\", x_train, '\\n')\n",
    "        \n",
    "        # znorm\n",
    "        std_ = x_train.std(axis=1, keepdims=True)\n",
    "        std_[std_ == 0] = 1.0\n",
    "        x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n",
    "\n",
    "        std_ = x_test.std(axis=1, keepdims=True)\n",
    "        std_[std_ == 0] = 1.0\n",
    "        x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n",
    "        \n",
    "        print(\"z_norm x_train:\", x_train, '\\n')\n",
    "        \n",
    "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                       y_test.copy())\n",
    "\n",
    "    return datasets_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vary length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x_train: [[ 0.     0.038  0.038 ...    nan    nan    nan]\n",
      " [-0.962 -0.962 -0.962 ...    nan    nan    nan]\n",
      " [ 0.423  0.538  0.5   ...    nan    nan    nan]\n",
      " ...\n",
      " [ 0.038  0.     0.038 ...    nan    nan    nan]\n",
      " [-0.115 -0.231 -0.115 ...    nan    nan    nan]\n",
      " [ 0.154  0.115  0.115 ...    nan    nan    nan]] \n",
      "\n",
      "z_norm x_train: [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] \n",
      "\n",
      "original x_train: [[ 0.     0.038  0.038 ...    nan    nan    nan]\n",
      " [-0.962 -0.962 -0.962 ...    nan    nan    nan]\n",
      " [ 0.423  0.538  0.5   ...    nan    nan    nan]\n",
      " ...\n",
      " [ 0.038  0.     0.038 ...    nan    nan    nan]\n",
      " [-0.115 -0.231 -0.115 ...    nan    nan    nan]\n",
      " [ 0.154  0.115  0.115 ...    nan    nan    nan]] \n",
      "\n",
      "nan to 0: [[ 0.     0.038  0.038 ...  0.     0.     0.   ]\n",
      " [-0.962 -0.962 -0.962 ...  0.     0.     0.   ]\n",
      " [ 0.423  0.538  0.5   ...  0.     0.     0.   ]\n",
      " ...\n",
      " [ 0.038  0.     0.038 ...  0.     0.     0.   ]\n",
      " [-0.115 -0.231 -0.115 ...  0.     0.     0.   ]\n",
      " [ 0.154  0.115  0.115 ...  0.     0.     0.   ]] \n",
      "\n",
      "z_norm x_train: [[-0.44613952 -0.05304682 -0.05304682 ... -0.44613952 -0.44613952\n",
      "  -0.44613952]\n",
      " [-1.76852885 -1.76852885 -1.76852885 ...  0.63900164  0.63900164\n",
      "   0.63900164]\n",
      " [ 4.73360396  6.0952572   5.64531961 ... -0.27491187 -0.27491187\n",
      "  -0.27491187]\n",
      " ...\n",
      " [ 0.45535108 -0.1643517   0.45535108 ... -0.1643517  -0.1643517\n",
      "  -0.1643517 ]\n",
      " [-1.04867495 -2.62661162 -1.04867495 ...  0.51565882  0.51565882\n",
      "   0.51565882]\n",
      " [ 1.42675073  0.98651402  0.98651402 ... -0.31161986 -0.31161986\n",
      "  -0.31161986]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AllGestureWiimoteX': (array([[-0.44613952, -0.05304682, -0.05304682, ..., -0.44613952,\n",
       "          -0.44613952, -0.44613952],\n",
       "         [-1.76852885, -1.76852885, -1.76852885, ...,  0.63900164,\n",
       "           0.63900164,  0.63900164],\n",
       "         [ 4.73360396,  6.0952572 ,  5.64531961, ..., -0.27491187,\n",
       "          -0.27491187, -0.27491187],\n",
       "         ...,\n",
       "         [ 0.45535108, -0.1643517 ,  0.45535108, ..., -0.1643517 ,\n",
       "          -0.1643517 , -0.1643517 ],\n",
       "         [-1.04867495, -2.62661162, -1.04867495, ...,  0.51565882,\n",
       "           0.51565882,  0.51565882],\n",
       "         [ 1.42675073,  0.98651402,  0.98651402, ..., -0.31161986,\n",
       "          -0.31161986, -0.31161986]]),\n",
       "  array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10.]),\n",
       "  array([[ 0.03047871,  0.43478815,  0.43478815, ..., -0.36346382,\n",
       "          -0.36346382, -0.36346382],\n",
       "         [-0.61113601, -0.42137324, -0.42137324, ..., -0.42137324,\n",
       "          -0.42137324, -0.42137324],\n",
       "         [ 0.14665813,  0.14665813, -1.26050945, ..., -0.20283447,\n",
       "          -0.20283447, -0.20283447],\n",
       "         ...,\n",
       "         [ 3.99013512,  3.99013512,  3.40244991, ..., -0.17005754,\n",
       "          -0.17005754, -0.17005754],\n",
       "         [ 0.64778817,  0.3773486 ,  0.3773486 , ..., -0.71864333,\n",
       "          -0.71864333, -0.71864333],\n",
       "         [-3.1795391 , -5.03161841, -6.41466464, ...,  0.05558643,\n",
       "           0.05558643,  0.05558643]]),\n",
       "  array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "          2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "          3.,  3.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "          4.,  4.,  4.,  4.,  4.,  4.,  4.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "          5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "          6.,  6.,  6.,  6.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "          7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "          8.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "          9.,  9.,  9.,  9.,  9.,  9., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "         10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.]))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the original data and change the nan value\n",
    "# set the archive_name and import the training dataset\n",
    "\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'AllGestureWiimoteX'\n",
    "read_dataset_original(root_dir, archive_name, dataset_name)\n",
    "\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'AllGestureWiimoteX'\n",
    "read_dataset_nan(root_dir, archive_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x_train: [[ 0.     0.038  0.038 ...    nan    nan    nan]\n",
      " [-0.962 -0.962 -0.962 ...    nan    nan    nan]\n",
      " [ 0.423  0.538  0.5   ...    nan    nan    nan]\n",
      " ...\n",
      " [ 0.038  0.     0.038 ...    nan    nan    nan]\n",
      " [-0.115 -0.231 -0.115 ...    nan    nan    nan]\n",
      " [ 0.154  0.115  0.115 ...    nan    nan    nan]] \n",
      "\n",
      "nan to 0: [[ 0.     0.038  0.038 ...  0.     0.     0.   ]\n",
      " [-0.962 -0.962 -0.962 ...  0.     0.     0.   ]\n",
      " [ 0.423  0.538  0.5   ...  0.     0.     0.   ]\n",
      " ...\n",
      " [ 0.038  0.     0.038 ...  0.     0.     0.   ]\n",
      " [-0.115 -0.231 -0.115 ...  0.     0.     0.   ]\n",
      " [ 0.154  0.115  0.115 ...  0.     0.     0.   ]] \n",
      "\n",
      "z_norm x_train: [[-0.44613952 -0.05304682 -0.05304682 ... -0.44613952 -0.44613952\n",
      "  -0.44613952]\n",
      " [-1.76852885 -1.76852885 -1.76852885 ...  0.63900164  0.63900164\n",
      "   0.63900164]\n",
      " [ 4.73360396  6.0952572   5.64531961 ... -0.27491187 -0.27491187\n",
      "  -0.27491187]\n",
      " ...\n",
      " [ 0.45535108 -0.1643517   0.45535108 ... -0.1643517  -0.1643517\n",
      "  -0.1643517 ]\n",
      " [-1.04867495 -2.62661162 -1.04867495 ...  0.51565882  0.51565882\n",
      "   0.51565882]\n",
      " [ 1.42675073  0.98651402  0.98651402 ... -0.31161986 -0.31161986\n",
      "  -0.31161986]] \n",
      "\n",
      "\t\titer 1\n",
      "\t\t\tdataset_name:  AllGestureWiimoteX\n",
      "\t\t\t\tDONE\n"
     ]
    }
   ],
   "source": [
    "# main Program for vary length \n",
    "# set the archive_name and import the training dataset\n",
    "# decide the classifier -- fcn or resnet\n",
    "\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'AllGestureWiimoteX'  \n",
    "classifier_name = 'fcn'\n",
    "# classifier_name = 'resnet'\n",
    "itr = '0'\n",
    "\n",
    "# iter = 1 is fcn classifier, iter = 2 is resnet classifier\n",
    "iter = 1\n",
    "\n",
    "# the different point: we use the read_dataset_nan\n",
    "datasets_dict = read_dataset_nan(root_dir, archive_name, dataset_name)\n",
    "\n",
    "\n",
    "print('\\t\\titer', iter)\n",
    "trr = ''\n",
    "if iter != 0:\n",
    "    trr = '_itr_' + str(iter)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "    print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    create_directory(output_directory)\n",
    "\n",
    "\n",
    "    \n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "    create_directory(output_directory + '/DONE')\n",
    "\n",
    "else:\n",
    "    print('iter = ', iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x_train: [[12. 18. 11. ...  6.  3.  6.]\n",
      " [12.  9. 11. ... 10.  4.  9.]\n",
      " [ 8.  5. 10. ...  7.  8.  6.]\n",
      " ...\n",
      " [11. 10.  8. ... 14.  9. 13.]\n",
      " [ 9.  4.  3. ... 14.  9. 11.]\n",
      " [16. 11.  5. ... 16. 21.  6.]] \n",
      "\n",
      "z_norm x_train: [[-0.41782779  0.21606722 -0.52347695 ... -1.05172279 -1.3686703\n",
      "  -1.05172279]\n",
      " [-0.39967471 -0.76278612 -0.52071185 ... -0.64174898 -1.3679718\n",
      "  -0.76278612]\n",
      " [-0.80742402 -1.11326108 -0.60353264 ... -0.90936971 -0.80742402\n",
      "  -1.0113154 ]\n",
      " ...\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [-0.80444756 -1.28115722 -1.37649916 ... -0.32773789 -0.80444756\n",
      "  -0.61376369]\n",
      " [-0.23282975 -0.65938808 -1.17125806 ... -0.23282975  0.19372857\n",
      "  -1.0859464 ]] \n",
      "\n",
      "original x_train: [[12. 18. 11. ...  6.  3.  6.]\n",
      " [12.  9. 11. ... 10.  4.  9.]\n",
      " [ 8.  5. 10. ...  7.  8.  6.]\n",
      " ...\n",
      " [11. 10.  8. ... 14.  9. 13.]\n",
      " [ 9.  4.  3. ... 14.  9. 11.]\n",
      " [16. 11.  5. ... 16. 21.  6.]] \n",
      "\n",
      "nan to 0: [[12. 18. 11. ...  6.  3.  6.]\n",
      " [12.  9. 11. ... 10.  4.  9.]\n",
      " [ 8.  5. 10. ...  7.  8.  6.]\n",
      " ...\n",
      " [11. 10.  8. ... 14.  9. 13.]\n",
      " [ 9.  4.  3. ... 14.  9. 11.]\n",
      " [16. 11.  5. ... 16. 21.  6.]] \n",
      "\n",
      "z_norm x_train: [[-0.41782779  0.21606722 -0.52347695 ... -1.05172279 -1.3686703\n",
      "  -1.05172279]\n",
      " [-0.39967471 -0.76278612 -0.52071185 ... -0.64174898 -1.3679718\n",
      "  -0.76278612]\n",
      " [-0.80742402 -1.11326108 -0.60353264 ... -0.90936971 -0.80742402\n",
      "  -1.0113154 ]\n",
      " ...\n",
      " [-0.74013875 -0.81530094 -0.96562531 ... -0.51465219 -0.89046312\n",
      "  -0.58981438]\n",
      " [-0.80444756 -1.28115722 -1.37649916 ... -0.32773789 -0.80444756\n",
      "  -0.61376369]\n",
      " [-0.23282975 -0.65938808 -1.17125806 ... -0.23282975  0.19372857\n",
      "  -1.0859464 ]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DodgerLoopDay': (array([[-0.41782779,  0.21606722, -0.52347695, ..., -1.05172279,\n",
       "          -1.3686703 , -1.05172279],\n",
       "         [-0.39967471, -0.76278612, -0.52071185, ..., -0.64174898,\n",
       "          -1.3679718 , -0.76278612],\n",
       "         [-0.80742402, -1.11326108, -0.60353264, ..., -0.90936971,\n",
       "          -0.80742402, -1.0113154 ],\n",
       "         ...,\n",
       "         [-0.74013875, -0.81530094, -0.96562531, ..., -0.51465219,\n",
       "          -0.89046312, -0.58981438],\n",
       "         [-0.80444756, -1.28115722, -1.37649916, ..., -0.32773789,\n",
       "          -0.80444756, -0.61376369],\n",
       "         [-0.23282975, -0.65938808, -1.17125806, ..., -0.23282975,\n",
       "           0.19372857, -1.0859464 ]]),\n",
       "  array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4., 4.,\n",
       "         4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "         5., 5., 5., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 7.,\n",
       "         7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]),\n",
       "  array([[-1.20833688, -1.20833688, -1.20833688, ..., -0.97357118,\n",
       "          -0.66055025, -1.05182641],\n",
       "         [-0.64914639,  0.50215824, -0.4720226 , ..., -0.73770828,\n",
       "          -0.82627018, -0.56058449],\n",
       "         [ 0.04753115, -0.29469315, -0.03802492, ..., -0.63691745,\n",
       "          -0.72247353, -1.49247821],\n",
       "         ...,\n",
       "         [-0.82770753, -1.00995506, -1.00995506, ..., -0.46321247,\n",
       "          -0.55433624, -1.10107883],\n",
       "         [ 0.2613434 , -0.16826219, -1.02747337, ..., -0.08234107,\n",
       "          -0.85563113, -0.6837889 ],\n",
       "         [-0.19894526, -0.71668232, -0.5441033 , ..., -0.11265575,\n",
       "          -0.37152428,  0.3187918 ]]),\n",
       "  array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4.,\n",
       "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5., 5.,\n",
       "         5., 5., 5., 5., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
       "         6., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]))}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the original data and change the nan value\n",
    "# set the archive_name and import the training dataset\n",
    "\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'DodgerLoopDay'\n",
    "read_dataset_original(root_dir, archive_name, dataset_name)\n",
    "\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'DodgerLoopDay'\n",
    "read_dataset_nan(root_dir, archive_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x_train: [[12. 18. 11. ...  6.  3.  6.]\n",
      " [12.  9. 11. ... 10.  4.  9.]\n",
      " [ 8.  5. 10. ...  7.  8.  6.]\n",
      " ...\n",
      " [11. 10.  8. ... 14.  9. 13.]\n",
      " [ 9.  4.  3. ... 14.  9. 11.]\n",
      " [16. 11.  5. ... 16. 21.  6.]] \n",
      "\n",
      "nan to 0: [[12. 18. 11. ...  6.  3.  6.]\n",
      " [12.  9. 11. ... 10.  4.  9.]\n",
      " [ 8.  5. 10. ...  7.  8.  6.]\n",
      " ...\n",
      " [11. 10.  8. ... 14.  9. 13.]\n",
      " [ 9.  4.  3. ... 14.  9. 11.]\n",
      " [16. 11.  5. ... 16. 21.  6.]] \n",
      "\n",
      "z_norm x_train: [[-0.41782779  0.21606722 -0.52347695 ... -1.05172279 -1.3686703\n",
      "  -1.05172279]\n",
      " [-0.39967471 -0.76278612 -0.52071185 ... -0.64174898 -1.3679718\n",
      "  -0.76278612]\n",
      " [-0.80742402 -1.11326108 -0.60353264 ... -0.90936971 -0.80742402\n",
      "  -1.0113154 ]\n",
      " ...\n",
      " [-0.74013875 -0.81530094 -0.96562531 ... -0.51465219 -0.89046312\n",
      "  -0.58981438]\n",
      " [-0.80444756 -1.28115722 -1.37649916 ... -0.32773789 -0.80444756\n",
      "  -0.61376369]\n",
      " [-0.23282975 -0.65938808 -1.17125806 ... -0.23282975  0.19372857\n",
      "  -1.0859464 ]] \n",
      "\n",
      "\t\titer 1\n",
      "\t\t\tdataset_name:  DodgerLoopDay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tDONE\n"
     ]
    }
   ],
   "source": [
    "# main Program for missing value\n",
    "# set the archive_name and import the training dataset\n",
    "# decide the classifier -- fcn or resnet\n",
    "\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'DodgerLoopDay'  \n",
    "classifier_name = 'fcn'\n",
    "# classifier_name = 'resnet'\n",
    "itr = '0'\n",
    "\n",
    "# iter = 1 is fcn classifier, iter = 2 is resnet classifier\n",
    "iter = 1\n",
    "\n",
    "# the different point: we use the read_dataset_nan\n",
    "datasets_dict = read_dataset_nan(root_dir, archive_name, dataset_name)\n",
    "\n",
    "\n",
    "print('\\t\\titer', iter)\n",
    "trr = ''\n",
    "if iter != 0:\n",
    "    trr = '_itr_' + str(iter)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "    print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    create_directory(output_directory)\n",
    "\n",
    "\n",
    "    \n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "    create_directory(output_directory + '/DONE')\n",
    "\n",
    "else:\n",
    "    print('iter = ', iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Multivariate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we cannot direct to use the multivariate dataset, we should definition the function to change the data type\n",
    "# After this function can we use the main program to get the result\n",
    "\n",
    "def transform_mts_to_ucr_format():\n",
    "   \n",
    "    # import our dataset\n",
    "    mts_root_dir = 'D:/FH_Dortmund/Projekt/Projektarbeit_2/time_series/dl-4-tsc-master/dataset/archives/mts_data/'\n",
    "    mts_out_dir = 'D:/FH_Dortmund/Projekt/Projektarbeit_2/time_series/dl-4-tsc-master/dataset/archives/mts_archive/'\n",
    "    \n",
    "    for dataset_name in MTS_DATASET_NAMES:\n",
    "        print('dataset_name',dataset_name)\n",
    "\n",
    "        out_dir = mts_out_dir + dataset_name + '/'\n",
    "\n",
    "        if create_directory(out_dir) is None:\n",
    "             print('Already_done')\n",
    "             continue\n",
    "\n",
    "        a = loadmat(mts_root_dir + dataset_name + '/' + dataset_name + '.mat')\n",
    "        a = a['mts']\n",
    "        a = a[0, 0]\n",
    "\n",
    "        dt = a.dtype.names\n",
    "        dt = list(dt)\n",
    "\n",
    "        for i in range(len(dt)):\n",
    "            if dt[i] == 'train':\n",
    "                x_train = a[i].reshape(max(a[i].shape))\n",
    "            elif dt[i] == 'test':\n",
    "                x_test = a[i].reshape(max(a[i].shape))\n",
    "            elif dt[i] == 'trainlabels':\n",
    "                y_train = a[i].reshape(max(a[i].shape))\n",
    "            elif dt[i] == 'testlabels':\n",
    "                y_test = a[i].reshape(max(a[i].shape))\n",
    "\n",
    "        n_var = x_train[0].shape[0]\n",
    "\n",
    "        max_length = get_func_length(x_train, x_test, func=max)\n",
    "        min_length = get_func_length(x_train, x_test, func=min)\n",
    "\n",
    "        print(dataset_name, 'max', max_length, 'min', min_length)\n",
    "        print()\n",
    "        # continue\n",
    "\n",
    "        x_train = transform_to_same_length(x_train, n_var, max_length)\n",
    "        x_test = transform_to_same_length(x_test, n_var, max_length)\n",
    "\n",
    "        # save them\n",
    "        np.save(out_dir + 'x_train.npy', x_train)\n",
    "        np.save(out_dir + 'y_train.npy', y_train)\n",
    "        np.save(out_dir + 'x_test.npy', x_test)\n",
    "        np.save(out_dir + 'y_test.npy', y_test)\n",
    "\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main Program for MTS\n",
    "# set the archive_name and import the training dataset\n",
    "# decide the classifier -- fcn or resnet\n",
    "archive_name = \"UCRArchive_2018\"\n",
    "dataset_name = 'ArabicDigits'\n",
    "classifier_name = 'fcn'\n",
    "# classifier_name = 'resnet'\n",
    "itr = '0'\n",
    "\n",
    "# iter = 1 is fcn classifier, iter = 2 is resnet classifier\n",
    "iter = 1\n",
    "\n",
    "# read the training data\n",
    "datasets_dict = read_all_datasets(root_dir, archive_name)\n",
    "\n",
    "\n",
    "print('\\t\\titer', iter)\n",
    "trr = ''\n",
    "\n",
    "# create the file about the result\n",
    "if iter != 0:\n",
    "    trr = '_itr_' + str(iter)\n",
    "\n",
    "    tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n",
    "\n",
    "    print('\\t\\t\\tdataset_name: ', dataset_name)\n",
    "\n",
    "    output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "    create_directory(output_directory)\n",
    "\n",
    "\n",
    "# fit the data    \n",
    "    fit_classifier()\n",
    "\n",
    "    print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "        # the creation of this directory means\n",
    "    create_directory(output_directory + '/DONE')\n",
    "\n",
    "else:\n",
    "    print('iter = ', iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
